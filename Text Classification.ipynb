{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Inbuilt Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import os,sys\n",
    "from sklearn import model_selection\n",
    "import re,string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word=[\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"as\",\"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\n",
    "\"by\",\"could\",\"did\",\"do\",\"does\",\"doing\",\"down\",\"during\",\"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"has\",\"have\",\"having\",\"he\",\"he'd\",\"he'll\",\"he's\",\"her\",\n",
    "\"here\",\"here's\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"how's\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"if\",\"in\",\"into\",\"is\",\"it\",\"it's\",\"its\",\"itself\",\"let's\",\"me\",\n",
    "\"more\",\"most\",\"my\",\"myself\",\"nor\",\"of\",\"on\",\"once\",\"only\",\"or\",\"other\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"same\",\"she\",\n",
    "\"she'd\",\"she'll\",\"she's\",\"should\",\"so\",\"some\",\"such\",\"than\",\"that\",\"that's\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"there's\",\n",
    "\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"this\",\"those\",\"through\",\"to\",\"too\",\"under\",\"until\",\"up\",\"very\",\"was\",\"we\",\"we'd\",\n",
    "\"we'll\",\"we're\",\"we've\",\"were\",\"what\",\"what's\",\"when\",\"when's\",\"where\",\"where's\",\"which\",\"while\",\"who\",\"who's\",\"whom\",\"why\",\"why's\",\"with\",\n",
    "\"would\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  =[] \n",
    "Y = []\n",
    "for category in os.listdir(\"D:\\\\Text Classification\\\\Datasets\"):\n",
    "    for document in os.listdir(\"D:\\\\Text Classification\\\\Datasets\\\\\"+category):\n",
    "        with open(\"D:\\\\Text Classification\\\\Datasets\\\\\"+category+'/'+document, \"r\") as f:\n",
    "            X.append((document,f.read()))\n",
    "            Y.append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X))\n",
    "print(type(X[0]))\n",
    "print(type(X[0][0]))\n",
    "print(type(X[0][1]))\n",
    "print(type(Y))\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=model_selection.train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=\"Hey! I am Yatin. I am Good.\"\n",
    "print(re.split(r'\\W+',sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={}\n",
    "for i in range(len(x_train)):\n",
    "    word=x_train[i][1].lower()\n",
    "    stripped=re.split(r'\\W+',word)\n",
    "    for s in stripped:\n",
    "        if not(s.isalpha()) or s in stop_word or len(s)<=2:\n",
    "            continue\n",
    "        if s in dic:\n",
    "            dic[s]+=1\n",
    "        else:\n",
    "            dic[s]=1\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dic = sorted(dic.items(), key=operator.itemgetter(1),reverse=True)\n",
    "sorted_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=sorted_dic\n",
    "answer1=[]\n",
    "answer2=[]\n",
    "for i in range(len(features)):\n",
    "    answer1.append(i)\n",
    "    answer2.append(features[i][1])\n",
    "plt.plot(answer1,answer2)\n",
    "plt.axis([0,8000,1,5000])\n",
    "plt.grid()\n",
    "plt.show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer1=[features[i][0] for i in range(2000)]\n",
    "answer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dataset=np.zeros([len(x_train),len(answer1)],int)\n",
    "for i in range(len(x_train)):\n",
    "    words=x_train[i][1].lower()\n",
    "    word=re.split(r'\\W+',words)\n",
    "    for j in word:\n",
    "        if j in answer1:\n",
    "            x_train_dataset[i][answer1.index(j)]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_dataset=np.zeros([len(x_test),len(answer1)],int)\n",
    "for i in range(len(x_test)):\n",
    "    words=x_test[i][1].lower()\n",
    "    word=re.split(r'\\W+',words)\n",
    "    for j in word:\n",
    "        if j in answer1:\n",
    "            x_test_dataset[i][answer1.index(j)]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_dataset)\n",
    "print(\"--------------------------\")\n",
    "print(x_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=MultinomialNB()\n",
    "clf.fit(x_train_dataset,y_train)\n",
    "y_pred=clf.predict(x_test_dataset)\n",
    "print(\"Score on training data:\",clf.score(x_train_dataset,y_train))\n",
    "print(\"Score on testing data:\",clf.score(x_test_dataset,y_test))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Implementation of Naive Baye's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(x_train_dataset,y_train):\n",
    "    count={}\n",
    "    total_word=0\n",
    "    y_train=np.array(y_train)\n",
    "    count[\"total_doc\"]=len(y_train)\n",
    "    classes=set(y_train)\n",
    "    for i in classes:\n",
    "        temp=0\n",
    "        x_train_with_i=x_train_dataset[y_train==i]\n",
    "        temp2=x_train_with_i.shape[0]\n",
    "        count[i]={}\n",
    "        for feature in answer1:\n",
    "            l=(x_train_with_i[:,answer1.index(feature)]).sum()\n",
    "            count[i][feature]=l\n",
    "            temp+=l\n",
    "        count[i][\"word_in_class\"]=temp\n",
    "        count[i][\"length\"]=temp2\n",
    "        \n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(x_test,dic,classes):\n",
    "    prob=np.log(dic[classes][\"length\"])-np.log(dic[\"total_doc\"])\n",
    "    feature=list(dic[classes].keys())\n",
    "    for j in range (len(feature)-2):\n",
    "        xj=x_test[j]\n",
    "        if xj==0:\n",
    "            current_prob=0\n",
    "        else:\n",
    "            num=dic[classes][feature[j]]+1\n",
    "            den=dic[classes][\"word_in_class\"]+len(dic[classes].keys())-2\n",
    "            current_prob=np.log(num)-np.log(den)\n",
    "        prob+=current_prob\n",
    "    return prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_single(x_test,dic):\n",
    "    first_run=True\n",
    "    classes=dic.keys()\n",
    "    for i in classes:\n",
    "        if i==\"total_doc\":\n",
    "            continue\n",
    "        prob=probability(x_test,dic,i)\n",
    "        if first_run or prob>best_prob:\n",
    "            best_prob=prob\n",
    "            first_run=False\n",
    "            best_class=i\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_(x_test,dic):\n",
    "    y_pred=[]\n",
    "    for x in x_test:\n",
    "        y_pred.append(predict_for_single(x,dic))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_test,y_pred):\n",
    "        count = 0\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == y_test[i]:\n",
    "                count+=1\n",
    "        return count/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=fit(x_train_dataset,y_train)\n",
    "y_pred=predict_(x_test_dataset,dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score on testing_data:\",score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9ba4ea07e24f00854dc55fefc3bba2478247dc8202d2afc1dac9d5323b8789f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
